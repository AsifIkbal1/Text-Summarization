{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "toc": {
      "nav_menu": {
        "height": "263px",
        "width": "352px"
      },
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "Copy of GRU.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaG-MmK-BxR_"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf \n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords \n",
        "stop = stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Iblv-HtABxSE"
      },
      "source": [
        "train = pd.read_csv(\"../Class11/Datasets/reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TD1QYLVhBxSF",
        "outputId": "3fc7074b-020a-4138-adb9-64d2e29af080"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>ProductId</th>\n",
              "      <th>UserId</th>\n",
              "      <th>ProfileName</th>\n",
              "      <th>HelpfulnessNumerator</th>\n",
              "      <th>HelpfulnessDenominator</th>\n",
              "      <th>Score</th>\n",
              "      <th>Time</th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>B001E4KFG0</td>\n",
              "      <td>A3SGXH7AUHU8GW</td>\n",
              "      <td>delmartian</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1303862400</td>\n",
              "      <td>Good Quality Dog Food</td>\n",
              "      <td>I have bought several of the Vitality canned d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>B00813GRG4</td>\n",
              "      <td>A1D87F6ZCVE5NK</td>\n",
              "      <td>dll pa</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1346976000</td>\n",
              "      <td>Not as Advertised</td>\n",
              "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>B000LQOCH0</td>\n",
              "      <td>ABXLMWJIXXAIN</td>\n",
              "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1219017600</td>\n",
              "      <td>\"Delight\" says it all</td>\n",
              "      <td>This is a confection that has been around a fe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>B000UA0QIQ</td>\n",
              "      <td>A395BORC6FGVXV</td>\n",
              "      <td>Karl</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1307923200</td>\n",
              "      <td>Cough Medicine</td>\n",
              "      <td>If you are looking for the secret ingredient i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>B006K2ZZ7K</td>\n",
              "      <td>A1UQRSCLF8GW1T</td>\n",
              "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>1350777600</td>\n",
              "      <td>Great taffy</td>\n",
              "      <td>Great taffy at a great price.  There was a wid...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Id   ProductId          UserId                      ProfileName  \\\n",
              "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
              "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
              "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
              "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
              "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
              "\n",
              "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
              "0                     1                       1      5  1303862400   \n",
              "1                     0                       0      1  1346976000   \n",
              "2                     1                       1      4  1219017600   \n",
              "3                     3                       3      2  1307923200   \n",
              "4                     0                       0      5  1350777600   \n",
              "\n",
              "                 Summary                                               Text  \n",
              "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
              "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
              "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
              "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
              "4            Great taffy  Great taffy at a great price.  There was a wid...  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8vcDai6BxSI"
      },
      "source": [
        "train = train[['Summary','Text']] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIWJDvfABxSJ"
      },
      "source": [
        "train['text_length'] = train['Text'].str.count(' ') #train er vitor kto gulo string ase ta count korbo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wve8vs8UBxSK",
        "outputId": "be149ce1-6370-4daa-938c-27958469420f"
      },
      "source": [
        "train['text_length'].describe() #describe er bapper ta box plot related."
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count    568454.000000\n",
              "mean         81.005522\n",
              "std          80.807102\n",
              "min           2.000000\n",
              "25%          33.000000\n",
              "50%          57.000000\n",
              "75%          99.000000\n",
              "max        3525.000000\n",
              "Name: text_length, dtype: float64"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxoX9B3iBxSK",
        "outputId": "800e35c9-1a28-4066-ba66-f9cbb6059834"
      },
      "source": [
        "train['summary_length'] = train['Summary'].str.count(' ')\n",
        "train['summary_length'].describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count    568427.000000\n",
              "mean          3.128462\n",
              "std           2.619420\n",
              "min           0.000000\n",
              "25%           1.000000\n",
              "50%           3.000000\n",
              "75%           4.000000\n",
              "max          41.000000\n",
              "Name: summary_length, dtype: float64"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZQH5NzrBxSL"
      },
      "source": [
        "train = train.loc[train['summary_length']<8] #<8-->summary_length  er  word jodi 8 er cheye choto hoy tahole oi data nia kaj korbe "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgogisgjMEY7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_R6PD7mBxSM"
      },
      "source": [
        "train = train.loc[train['text_length']<30] #<30-->summary_length  er  word jodi 30 er cheye choto hoy tahole oi data nia kaj korbe "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gulPVmAJDlZ"
      },
      "source": [
        "#iloc=index location\n",
        "#loc=location\n",
        "#'' --> space nai\n",
        "#' '---->space ase"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZzu8LTuBxSN",
        "outputId": "88394ad4-11ea-411d-bbf1-ddb99f6b2b7c"
      },
      "source": [
        "print(train.shape)\n",
        "print(train.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(109053, 4)\n",
            "                   Summary                                               Text  \\\n",
            "4              Great taffy  Great taffy at a great price.  There was a wid...   \n",
            "7   Wonderful, tasty taffy  This taffy is so good.  It is very soft and ch...   \n",
            "8               Yay Barley  Right now I'm mostly just sprouting this so my...   \n",
            "9         Healthy Dog Food  This is a very healthy dog food. Good for thei...   \n",
            "13       fresh and greasy!  good flavor! these came securely packed... the...   \n",
            "\n",
            "    text_length  summary_length  \n",
            "4            29             1.0  \n",
            "7            27             2.0  \n",
            "8            25             1.0  \n",
            "9            24             2.0  \n",
            "13           14             2.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjDO9aH6BxSO",
        "outputId": "3c4a5c12-3c94-4810-f096-597399e50241"
      },
      "source": [
        "train['text_lower'] = train['Text'].str.lower() #small letter e korlm/ str---> nilam karon amra sudhu string gulo nia kaj korbo.\n",
        "train['text_no_punctuation'] = train['text_lower'].str.replace('[^\\w\\s]','') #[^\\w\\s]----->white space"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/DL/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18eTfK4fBxSO",
        "outputId": "28819a1b-d50f-4d7c-ec8a-ac729539fe2e"
      },
      "source": [
        "train['summary_lower'] = train[\"Summary\"].str.lower()\n",
        "train['summary_no_punctuation'] =  '_start_' + ' ' +train['summary_lower'].str.replace('[^\\w\\s]','')+ ' ' +'_end_' #_start_/'_end_--->protita summary er ses e blank space thkbe ,"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/DL/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPJbrniABxSP"
      },
      "source": [
        "max_features1 = 5000 #sudhu main word/token gulo nibe  jar poriman hbe 5000\n",
        "maxlen1 = 30 #maximum lenth 30 hbe (text_length)\n",
        "\n",
        "max_features2 = 5000 \n",
        "maxlen2 = 8 ##maximum lenth 8 hbe (summary_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMBF2ZefBxSP"
      },
      "source": [
        "tok1 = tf.keras.preprocessing.text.Tokenizer(num_words=max_features1) \n",
        "tok1.fit_on_texts(list(train['text_no_punctuation'].astype(str))) #fit to cleaned text(text_no_punctuation)kno punction thkbe na ,even number thkle ta string e conv. hbe \n",
        "tf_train_text =tok1.texts_to_sequences(list(train['text_no_punctuation'].astype(str))) #astype(str)---> ja ase sob str e conv hbe\n",
        "tf_train_text =tf.keras.preprocessing.sequence.pad_sequences(tf_train_text, maxlen=maxlen1) #let's execute pad step //"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ixpCQztBxSQ"
      },
      "source": [
        "tok2 = tf.keras.preprocessing.text.Tokenizer(num_words=max_features2, filters = '*') #------>\n",
        "tok2.fit_on_texts(list(train['summary_no_punctuation'].astype(str))) #fit to cleaned text\n",
        "tf_train_summary = tok2.texts_to_sequences(list(train['summary_no_punctuation'].astype(str)))\n",
        "tf_train_summary = tf.keras.preprocessing.sequence.pad_sequences(tf_train_summary, maxlen=maxlen2, padding ='post')#padding ='post'---->layer gula change hbe,sme hole change hto na//pad_sequences--->RNN vlo vbe kaj korbe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5enHVE8BxSQ"
      },
      "source": [
        "# Define Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ms6uhuYKBxSS"
      },
      "source": [
        "#vectorized\n",
        "\n",
        "vectorized_summary = tf_train_summary  #single array te convert kora/machine related e conv kora\n",
        "decoder_input_data = vectorized_summary[:, :-1]\n",
        "decoder_target_data = vectorized_summary[:, 1:]\n",
        "#summary main bkz oi ta dia kaj hbe tai oi ta decode korlm\n",
        "\n",
        "vectorized_text = tf_train_text\n",
        "encoder_input_data = vectorized_text #eta decode na korleo cholbe\n",
        "doc_length = encoder_input_data.shape[1] #shape[1]--->index ke access korteci/1 number sari ta access korteci"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kli0eL2EBxST"
      },
      "source": [
        "vocab_size_encoder = len(tok1.word_index) + 1 #last indes jate miss na hoy ,ti 1 jog korlm\n",
        "vocab_size_decoder = len(tok2.word_index) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXg1eznEBxST"
      },
      "source": [
        "### Define Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bt32cimABxSU"
      },
      "source": [
        "#arbitrarly set latent dimension for embedding and hidden units\n",
        "latent_dim = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9etveKxHBxSV"
      },
      "source": [
        "encoder_inputs = tf.keras.Input(shape=(doc_length,), name='Encoder-Input') #shape=(doc_length)----> vectorized value (18 number columns)\n",
        "\n",
        "x = tf.keras.layers.Embedding(vocab_size_encoder, #(vocab_size_encoder)---->#Encoding value (19 number columns)\n",
        "                              latent_dim, name='Body-Word-Embedding',#miss kisu thkle ta fillup korbe/latent_dim--->(20 number columns)\n",
        "                              mask_zero=False)(encoder_inputs) #mask_zero=False---->kno index er value jdi missing thake ta fill up korbo na\n",
        "\n",
        "x = tf.keras.layers.BatchNormalization(name='Encoder-Batchnorm-1')(x)\n",
        "\n",
        "_, state_h = tf.keras.layers.GRU(latent_dim, #_,---> output ta skip korteci but state_h dekbo ti nisi\n",
        "                                 return_state=True, \n",
        "                                 name='Encoder-Last-GRU')(x)\n",
        "\n",
        "encoder_model = tf.keras.Model(inputs=encoder_inputs,\n",
        "                               outputs=state_h, \n",
        "                               name='Encoder-Model')\n",
        "\n",
        "seq2seq_encoder_out = encoder_model(encoder_inputs) #encoder_model(encoder_inputs)--> ta sequence hesabe nilm\n",
        "\n",
        "\n",
        "decoder_inputs = tf.keras.Input(shape=(None,),\n",
        "                                name='Decoder-Input')  # for teacher forcing\n",
        "\n",
        "dec_emb = tf.keras.layers.Embedding(vocab_size_decoder, #Encoding value (19 number columns)\n",
        "                                    latent_dim, #latent_dim--->(20 number columns)\n",
        "                                    name='Decoder-Word-Embedding',\n",
        "                                    mask_zero=False)(decoder_inputs)\n",
        "\n",
        "dec_bn = tf.keras.layers.BatchNormalization(name='Decoder-Batchnorm-1')(dec_emb)\n",
        "\n",
        "\n",
        "decoder_gru = tf.keras.layers.GRU(latent_dim, \n",
        "                                  return_state=True, \n",
        "                                  return_sequences=True, \n",
        "                                  name='Decoder-GRU')\n",
        "decoder_gru_output, _ = decoder_gru(dec_bn, \n",
        "                                    initial_state=seq2seq_encoder_out) \n",
        "x = tf.keras.layers.BatchNormalization(name='Decoder-Batchnorm-2')(decoder_gru_output)\n",
        "\n",
        "\n",
        "decoder_dense = tf.keras.layers.Dense(vocab_size_decoder,\n",
        "                                      activation='softmax',\n",
        "                                      name='Final-Output-Dense')\n",
        "decoder_outputs = decoder_dense(x)\n",
        "\n",
        "seq2seq_Model = tf.keras.Model([encoder_inputs\n",
        "                                , decoder_inputs], decoder_outputs)\n",
        "\n",
        "seq2seq_Model.compile(optimizer=tf.keras.optimizers.Nadam(learning_rate=0.001),\n",
        "                      loss='sparse_categorical_crossentropy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7Opsgq3BxSW"
      },
      "source": [
        "** Examine Model Architecture Summary **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n15DCY42BxSX"
      },
      "source": [
        "seq2seq_Model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eg-OKIgVBxSX"
      },
      "source": [
        "# Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lOxRBbmBxSX",
        "outputId": "d285ad89-b35d-4d9e-cf27-0cd6032d090f"
      },
      "source": [
        "batch_size = 64\n",
        "epochs = 2 \n",
        "history = seq2seq_Model.fit([encoder_input_data,\n",
        "                             decoder_input_data], \n",
        "                            np.expand_dims(\n",
        "                                decoder_target_data, -1),\n",
        "          batch_size=batch_size,  epochs=epochs ,  \n",
        "                            validation_split=0.12) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "1500/1500 [==============================] - 121s 78ms/step - loss: 2.6445 - val_loss: 1.9713\n",
            "Epoch 2/2\n",
            "1500/1500 [==============================] - 114s 76ms/step - loss: 1.7928 - val_loss: 1.7531\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGnHuRsuBxSY"
      },
      "source": [
        "test_text = ['this product is great  gives you so much energy and tastes great  try this cafe  latte and all the other flavors and you will not be disappointed']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IH-H4fjiBxSZ"
      },
      "source": [
        "tok1.fit_on_texts(test_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCKmP5qGBxSZ"
      },
      "source": [
        "raw_tokenized = tok1.texts_to_sequences(test_text)\n",
        "raw_tokenized = tf.keras.preprocessing.sequence.pad_sequences(raw_tokenized, maxlen=maxlen1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8aUKtVjBxSa"
      },
      "source": [
        "body_encoding = encoder_model.predict(raw_tokenized) #predict the encoder state of the new sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcslleYJBxSa"
      },
      "source": [
        "latent_dim = seq2seq_Model.get_layer('Decoder-Word-Embedding').output_shape[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KA1VzxArBxSb"
      },
      "source": [
        "#remember the get layer methodo for getting the embedding (word clusters)\n",
        "decoder_inputs = seq2seq_Model.get_layer('Decoder-Input').input \n",
        "dec_emb = seq2seq_Model.get_layer('Decoder-Word-Embedding')(decoder_inputs)\n",
        "dec_bn = seq2seq_Model.get_layer('Decoder-Batchnorm-1')(dec_emb)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKFiGPvfBxSb"
      },
      "source": [
        "gru_inference_state_input = tf.keras.Input(shape=(latent_dim,),\n",
        "                                           name='hidden_state_input')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ch3-_dnKBxSc"
      },
      "source": [
        "gru_out, gru_state_out = seq2seq_Model.get_layer('Decoder-GRU')([dec_bn, gru_inference_state_input])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxTaWUGoBxSc"
      },
      "source": [
        "# Reconstruct dense layers\n",
        "dec_bn2 = seq2seq_Model.get_layer('Decoder-Batchnorm-2')(gru_out)\n",
        "dense_out = seq2seq_Model.get_layer('Final-Output-Dense')(dec_bn2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgQkQTgKBxSc"
      },
      "source": [
        "decoder_model = tf.keras.Model([decoder_inputs, \n",
        "                                gru_inference_state_input],\n",
        "                          [dense_out, gru_state_out])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EpYR2DGBxSc"
      },
      "source": [
        "original_body_encoding = body_encoding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-XI_hb5BxSd"
      },
      "source": [
        "state_value = np.array(tok2.word_index['_start_']).reshape(1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnOfAM6hBxSd",
        "outputId": "0387b507-6f98-45dc-c803-7ffc5e575c85"
      },
      "source": [
        "state_value"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1]])"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1l5pOLNhBxSe"
      },
      "source": [
        "decoded_sentence = []\n",
        "stop_condition = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dsN31H4BxSe"
      },
      "source": [
        "vocabulary_inv = dict((v, k) for k, v in tok2.word_index.items())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yy1SpTPmBxSe"
      },
      "source": [
        "#print(vocabulary_inv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGXoWeU-BxSe",
        "outputId": "087bcdd8-add3-45c5-97fe-d09fabf0a189"
      },
      "source": [
        "while not stop_condition:\n",
        "    #print(1)\n",
        "    preds, st = decoder_model.predict([state_value, body_encoding])\n",
        "\n",
        "    pred_idx = np.argmax(preds[:, :, 2:]) + 2\n",
        "    pred_word_str = vocabulary_inv[pred_idx]\n",
        "    print(pred_word_str)\n",
        "    if pred_word_str == '_end_' or len(decoded_sentence) >= maxlen2:\n",
        "        stop_condition = True\n",
        "        break\n",
        "    decoded_sentence.append(pred_word_str)\n",
        "\n",
        "    # update the decoder for the next word\n",
        "    body_encoding = st\n",
        "    state_value = np.array(pred_idx).reshape(1, 1)\n",
        "    #print(state_value)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "great\n",
            "product\n",
            "_end_\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jZppe5jBxSf",
        "outputId": "a5ddaf31-4dbb-4a7d-961e-d3d859b1acd9"
      },
      "source": [
        "train.tail()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "      <th>text_length</th>\n",
              "      <th>summary_length</th>\n",
              "      <th>text_lower</th>\n",
              "      <th>text_no_punctuation</th>\n",
              "      <th>summary_lower</th>\n",
              "      <th>summary_no_punctuation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>568439</th>\n",
              "      <td>a-ok</td>\n",
              "      <td>We need this for a recipe my wife is intereste...</td>\n",
              "      <td>22</td>\n",
              "      <td>0.0</td>\n",
              "      <td>we need this for a recipe my wife is intereste...</td>\n",
              "      <td>we need this for a recipe my wife is intereste...</td>\n",
              "      <td>a-ok</td>\n",
              "      <td>_start_ aok _end_</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568442</th>\n",
              "      <td>Great Cafe Latte</td>\n",
              "      <td>This product is great.  Gives you so much ener...</td>\n",
              "      <td>28</td>\n",
              "      <td>2.0</td>\n",
              "      <td>this product is great.  gives you so much ener...</td>\n",
              "      <td>this product is great  gives you so much energ...</td>\n",
              "      <td>great cafe latte</td>\n",
              "      <td>_start_ great cafe latte _end_</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568448</th>\n",
              "      <td>Very large ground spice jars.</td>\n",
              "      <td>My only complaint is that there's so much of i...</td>\n",
              "      <td>28</td>\n",
              "      <td>4.0</td>\n",
              "      <td>my only complaint is that there's so much of i...</td>\n",
              "      <td>my only complaint is that theres so much of it...</td>\n",
              "      <td>very large ground spice jars.</td>\n",
              "      <td>_start_ very large ground spice jars _end_</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568449</th>\n",
              "      <td>Will not do without</td>\n",
              "      <td>Great for sesame chicken..this is a good if no...</td>\n",
              "      <td>25</td>\n",
              "      <td>3.0</td>\n",
              "      <td>great for sesame chicken..this is a good if no...</td>\n",
              "      <td>great for sesame chickenthis is a good if not ...</td>\n",
              "      <td>will not do without</td>\n",
              "      <td>_start_ will not do without _end_</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568453</th>\n",
              "      <td>Great Honey</td>\n",
              "      <td>I am very satisfied ,product is as advertised,...</td>\n",
              "      <td>20</td>\n",
              "      <td>1.0</td>\n",
              "      <td>i am very satisfied ,product is as advertised,...</td>\n",
              "      <td>i am very satisfied product is as advertised i...</td>\n",
              "      <td>great honey</td>\n",
              "      <td>_start_ great honey _end_</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                              Summary  \\\n",
              "568439                           a-ok   \n",
              "568442               Great Cafe Latte   \n",
              "568448  Very large ground spice jars.   \n",
              "568449            Will not do without   \n",
              "568453                    Great Honey   \n",
              "\n",
              "                                                     Text  text_length  \\\n",
              "568439  We need this for a recipe my wife is intereste...           22   \n",
              "568442  This product is great.  Gives you so much ener...           28   \n",
              "568448  My only complaint is that there's so much of i...           28   \n",
              "568449  Great for sesame chicken..this is a good if no...           25   \n",
              "568453  I am very satisfied ,product is as advertised,...           20   \n",
              "\n",
              "        summary_length                                         text_lower  \\\n",
              "568439             0.0  we need this for a recipe my wife is intereste...   \n",
              "568442             2.0  this product is great.  gives you so much ener...   \n",
              "568448             4.0  my only complaint is that there's so much of i...   \n",
              "568449             3.0  great for sesame chicken..this is a good if no...   \n",
              "568453             1.0  i am very satisfied ,product is as advertised,...   \n",
              "\n",
              "                                      text_no_punctuation  \\\n",
              "568439  we need this for a recipe my wife is intereste...   \n",
              "568442  this product is great  gives you so much energ...   \n",
              "568448  my only complaint is that theres so much of it...   \n",
              "568449  great for sesame chickenthis is a good if not ...   \n",
              "568453  i am very satisfied product is as advertised i...   \n",
              "\n",
              "                        summary_lower  \\\n",
              "568439                           a-ok   \n",
              "568442               great cafe latte   \n",
              "568448  very large ground spice jars.   \n",
              "568449            will not do without   \n",
              "568453                    great honey   \n",
              "\n",
              "                            summary_no_punctuation  \n",
              "568439                           _start_ aok _end_  \n",
              "568442              _start_ great cafe latte _end_  \n",
              "568448  _start_ very large ground spice jars _end_  \n",
              "568449           _start_ will not do without _end_  \n",
              "568453                   _start_ great honey _end_  "
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puoPHVyyBxSf",
        "outputId": "867eff4a-8640-4e4b-d505-165244c75a07"
      },
      "source": [
        "train['summary_no_punctuation'][568442]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'_start_ great cafe latte _end_'"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUFikSY0BxSf",
        "outputId": "a9c06cee-6f36-4384-8fcf-e0174be03829"
      },
      "source": [
        "train['Text'][568442]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'This product is great.  Gives you so much energy and tastes great.  Try this Cafe Latte and all the other flavors and you will not be disappointed.'"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDQ6BC8kBxSg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}